import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import time
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import recall_score, make_scorer
from sklearn.model_selection import StratifiedKFold
from skopt import BayesSearchCV
from skopt.space import Real, Categorical, Integer


# ðŸ“Œ DÃ©finir le fichier pour stocker la meilleure configuration
output_file = "bayes_optimization_results.txt"

# ðŸ“Œ Chemins des fichiers de donnÃ©es
data_train_path = "C:/Users/malle/OneDrive/Bureau/Algo_optimisation_hyperparam/als_train_t3.xlsx"

# ðŸ“Œ Charger les donnÃ©es d'entraÃ®nement
df_train = pd.read_excel(data_train_path)

# ðŸ“Œ SÃ©lection des variables et de la cible
X_train = df_train.drop(columns=['Survived'])
y_train = df_train['Survived']




# ðŸ“Œ DÃ©finir la fonction de prÃ©cision personnalisÃ©e
def custom_precision(y_true, y_pred):
    recall_class_1 = recall_score(y_true, y_pred, pos_label=1)
    recall_class_0 = recall_score(y_true, y_pred, pos_label=0)
    average_recall = (recall_class_1 + recall_class_0) / 2
    return average_recall


# ðŸ“Œ DÃ©finir l'espace des hyperparamÃ¨tres pour l'optimisation bayÃ©sienne
param_dist = {
    'n_estimators': (10, 200),
    'max_depth': (5, 30),
    'min_samples_split': (2, 12),
    'min_samples_leaf': (1, 12),
    'max_features': ['sqrt', 'log2'],
    'criterion': ['gini', 'entropy'],
    'class_weight': ['balanced','balanced_subsample'],
    'ccp_alpha': Categorical([0.003])
}



# ðŸ“Œ Initialiser la meilleure valeur
best_score = -np.inf  # Score initial bas pour garantir une amÃ©lioration
best_params = None

# ðŸ“Œ Liste pour stocker l'Ã©volution de la prÃ©cision personnalisÃ©e
custom_precision_history = []

# ðŸ“Œ DÃ©finir la limite de temps (en secondes)
time_limit = 7200  # 2 heures
start_time = time.time()


# ðŸ“Œ Fonction de callback pour surveiller le temps et sauvegarder la meilleure configuration
def on_step(optim_result):
    global best_score, best_params

    # VÃ©rifier si le temps limite est dÃ©passÃ©
    elapsed_time = time.time() - start_time
    if elapsed_time > time_limit:
        print("\nTemps limite dÃ©passÃ© ! Affichage du graphe...")

        # ðŸ“Š Affichage du graphique final
        plt.figure(figsize=(10, 6))
        plt.plot(range(1, len(custom_precision_history) + 1), custom_precision_history, marker='o', linestyle='-', color='blue',
                 label="Custom Precision")
        plt.xlabel("ItÃ©rations")
        plt.ylabel("Custom Precision")
        plt.title("Ã‰volution de la Custom Precision (Optimisation interrompue)")
        plt.legend()
        plt.grid(True)
        plt.show()

        return False  # Continuer l'optimisation, mais ne pas sauvegarder de nouvelles valeurs

    # RÃ©cupÃ©rer les derniers hyperparamÃ¨tres testÃ©s et leur score
    current_params = dict(zip(param_dist.keys(), optim_result.x))
    current_score = -optim_result.fun  # NÃ©gatif car BayesSearchCV minimise

    custom_precision_history.append(current_score)  # Stocke l'Ã©volution

    # Si cette configuration est meilleure, on met Ã  jour
    if current_score > best_score:
        best_score = current_score
        best_params = current_params

        # ðŸ“Œ Ã‰crire uniquement la meilleure configuration dans le fichier
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(f"Best Score: {best_score:.4f}\n")
            for key ,value in best_params.items():
                f.write(f" {value}\n")

        print(f"\nNouvelle meilleure configuration trouvÃ©e ! Custom Precision: {best_score:.4f}")
        print(f"HyperparamÃ¨tres: {best_params}\n")

    return False  # Continuer l'optimisation


# ðŸ“Œ Configuration de la recherche bayÃ©sienne
custom_scorer = make_scorer(custom_precision)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

bayes_search = BayesSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    search_spaces=param_dist,
    n_iter=50000,
    cv=cv,
    verbose=3,
    n_jobs=-1,
    random_state=42,
    scoring=custom_scorer,
    refit=True,
    return_train_score=True
)

# ðŸ“Œ ExÃ©cuter BayesSearchCV avec le callback
bayes_search.fit(X_train, y_train, callback=on_step)

# ðŸ“Œ Temps total d'exÃ©cution
elapsed_time = time.time() - start_time
print(f"Temps d'exÃ©cution : {elapsed_time / 60:.2f} minutes")

# ðŸ“Œ Sauvegarde finale si dans le temps limite
if elapsed_time <= time_limit and best_params:
    with open(output_file, "a", encoding="utf-8") as f:
        f.write(f"\nTemps d'exÃ©cution : {elapsed_time / 60:.2f} minutes\n")
    print("\nMeilleure configuration sauvegardÃ©e !")
else:
    print("\nAucune configuration sauvegardÃ©e car le temps limite a Ã©tÃ© dÃ©passÃ©.")

# ðŸ“Š Affichage du graphique si l'optimisation a pris toute la durÃ©e
if elapsed_time >= time_limit:
    print("\nAffichage du graphique car le temps limite a Ã©tÃ© atteint !")
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(custom_precision_history) + 1), custom_precision_history, marker='o', linestyle='-', color='blue',
             label="Custom Precision")
    plt.xlabel("ItÃ©rations")
    plt.ylabel("Custom Precision")
    plt.title("Ã‰volution de la Custom Precision (Temps limite atteint)")
    plt.legend()
    plt.grid(True)
    plt.show()
